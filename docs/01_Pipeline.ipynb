{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b4ee67f",
   "metadata": {},
   "source": [
    "# Kepler-ECG: Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e507a9",
   "metadata": {},
   "source": [
    "## FASE 0: Setup & Domain Understanding\n",
    "\n",
    "### Objective\n",
    "\n",
    "Configure environment, download data, understand domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82016bc",
   "metadata": {},
   "source": [
    "### Step 0.1: Create project structure\n",
    "\n",
    "```bash\n",
    "kepler-ecg/\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ ‚îî‚îÄ‚îÄ raw/              # Original ECG\n",
    "‚îú‚îÄ‚îÄ src/kepler_ecg/\n",
    "‚îÇ ‚îú‚îÄ‚îÄ preprocessing/    # Filters, quality, beat segmentation\n",
    "‚îÇ ‚îú‚îÄ‚îÄ features/         # Features Extractors\n",
    "‚îÇ ‚îî‚îÄ‚îÄ discovery/        # Symbolic regression\n",
    "‚îú‚îÄ‚îÄ scripts/            # Executable scripts\n",
    "‚îú‚îÄ‚îÄ results/            # Output analysis\n",
    "‚îî‚îÄ‚îÄ test/               # Unit tests\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354191cd",
   "metadata": {},
   "source": [
    "### Step 0.2: Install dependencies\n",
    "\n",
    "```bash\n",
    "# Core Data Science stack: math, signal tools, dataframes, and visualization\n",
    "pip install numpy scipy pandas matplotlib seaborn\n",
    "\n",
    "# Specialized Biomedical Signal Processing (essential for PhysioNet datasets)\n",
    "# wfdb: read/write PhysioNet files\n",
    "# neurokit2/biosppy: advanced ECG/EEG feature extraction and cleaning\n",
    "pip install wfdb neurokit2 biosppy\n",
    "\n",
    "# Symbolic Regression: Discovering mathematical expressions from data\n",
    "pip install pysr\n",
    "\n",
    "# Utilities and Performance:\n",
    "# tqdm: progress bars for loops\n",
    "# pyarrow: high-performance data storage and retrieval\n",
    "# pytest: framework for testing your preprocessing pipeline\n",
    "pip install tqdm pyarrow pytest\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619c3d95",
   "metadata": {},
   "source": [
    "### Step 0.3: Multi-Dataset Registry\n",
    "\n",
    "**Script**: [src/core/dataset_registry.py](../src/core/dataset_registry.py)\n",
    "\n",
    "The **Dataset Registry** is the central configuration engine that enables **Kepler-ECG** to support multiple heart-signal datasets (such as PTB-XL, Chapman, Georgia, and MIT-BIH) through a unified interface. It acts as a \"source of truth\" for signal properties, defining the sampling rates, lead configurations, and file formats (WFDB, MAT, CSV) for each source. By abstracting the complexities of different metadata structures and labeling schemas (SCP-Codes, SNOMED, etc.), it allows the rest of the pipeline to remain dataset-agnostic, facilitating large-scale cross-database research.\n",
    "\n",
    "#### Input\n",
    "\n",
    "The registry is primarily accessed via its internal configuration classes and utility functions:\n",
    "\n",
    "* **`dataset_name` (str)**: A unique identifier for the requested dataset (e.g., `'ptb-xl'`, `'mit-bih'`, or `'chapman'`).\n",
    "* **`data_path` (Path/str)**: The local system path to the raw dataset files.\n",
    "* **`_registry` (Internal)**: A singleton object containing predefined **`DatasetConfig`** dataclasses for every supported source.\n",
    "* **`auto_detect` (Utility)**: Can take a directory path and scan for specific metadata files (like `ptbxl_database.csv`) to identify the dataset automatically.\n",
    "\n",
    "#### Output\n",
    "\n",
    "The module provides structured configuration objects and metadata mapping:\n",
    "\n",
    "* **`DatasetConfig` Object**: A comprehensive container including:\n",
    "* **Signal Metadata**: Sampling rate (Hz), number of leads, and lead names.\n",
    "* **Format Specifiers**: The `FileFormat` enum (e.g., `WFDB`) and `LabelSource` enum.\n",
    "* **Processing Functions**: References to specific loader functions for that dataset's unique metadata structure.\n",
    "\n",
    "\n",
    "* **Registry Summary**: A descriptive string listing all available datasets and their basic parameters for logging purposes.\n",
    "* **Detection Result**: The identified dataset name string if the auto-detection utility is used.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dbadf5",
   "metadata": {},
   "source": [
    "### Step 0.4: Download dataset\n",
    "\n",
    "**Script**: [scripts/download_dataset.py](../scripts/download_dataset.py)\n",
    "\n",
    "Python utility designed to automate the download of clinical datasets from the **PhysioNet** platform. The script acts as a wrapper around the 'wget' tool, handling automatic directory creation and file organization mirroring the source, eliminating redundant server folder hierarchies (via the 'cut-dirs' option). It is optimized to be integrated into cross-platform search pipelines.\n",
    "\n",
    "#### Input (Commandline Arguments)\n",
    "\n",
    "The file accepts the following parameters via 'argparse':\n",
    "\n",
    "| Parameter | Type | Mandatory | Description |\n",
    "|-----------|------|-----------|-------------|\n",
    "| 'url' | String | **Required** | The direct URL of the dataset on PhysioNet (e.g. 'https://physionet.org/files/ptb-xl/1.0.3/`). |\n",
    "| '--output' | Path (Path) | Optional | The local destination folder. Default: 'data/raw'. |\n",
    "\n",
    "#### Output\n",
    "\n",
    "* **Local Files:** A mirror copy of the downloaded dataset in the specified folder (keeping the original structure of the '.dat', '.hea', '.csv', etc. files).\n",
    "* **Console:** Real-time log of download status and final confirmation of save path.\n",
    "* **Exit Status:** Returns '0' on success, '1' on error (e.g. 'wget' not installed or URL unreachable).\n",
    "\n",
    "\n",
    "#### Running the script for different datasets (e.g. ptb-xl)\n",
    "\n",
    "```bash\n",
    "# Download ptb-xl:\n",
    "python scripts/download_dataset.py --dataset ptb-xl\n",
    "\n",
    "# List of avaiable dataset:\n",
    "python scripts/download_dataset.py --list\n",
    "\n",
    "# Download all dataset:\n",
    "python scripts/download_dataset.py --all\n",
    "```\n",
    "\n",
    "**Output**: \n",
    "- Folders containing data and metadata\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ff31d7",
   "metadata": {},
   "source": [
    "## FASE 1: Preprocessing Pipeline\n",
    "\n",
    "### Objective\n",
    "\n",
    "Create robust pipeline: filters, quality assessment, R peak detection.\n",
    "\n",
    "### Step 1.1: BaselineRemover: High-pass Butterworth 0.5Hz & NoiseFilter: low-pass 40Hz + notch 50Hz\n",
    "\n",
    "**Script**: [src/preprocessing/filters.py](../src/preprocessing/filters.py)\n",
    "\n",
    "A specialized Python module for ECG signal preprocessing that implements high-performance filtering techniques. It provides tools to eliminate common cardiac signal artifacts, such as low-frequency baseline wander (caused by breathing or movement) and high-frequency noise (EMG interference or powerline hum). The module utilizes zero-phase filtering (`filtfilt`) to ensure that the morphological timing of the ECG waves (P, QRS, and T) remains undistorted, which is critical for accurate deep learning analysis.\n",
    "\n",
    "#### Input\n",
    "\n",
    "The classes (`BaselineRemover`, `NoiseFilter`, `ECGFilter`) and convenience functions accept the following primary inputs:\n",
    "\n",
    "* **`signal` (np.ndarray):** The raw ECG data. It can be 1D (a single lead) or 2D (multiple leads, shaped as `n_samples, n_leads`).\n",
    "* **`fs` (int):** The sampling frequency of the signal in Hz (e.g., 500 Hz for PTB-XL).\n",
    "* **Configuration Parameters:**\n",
    "* **Baseline Removal:** `baseline_cutoff` (default 0.5 Hz) and `baseline_order`.\n",
    "* **Noise Filtering:** `lowpass_cutoff` (default 40.0 Hz) and `lowpass_order`.\n",
    "* **Powerline Interference:** `notch_freq` (50 Hz for Europe/Italy or 60 Hz) and `notch_q` (quality factor).\n",
    "\n",
    "#### Output\n",
    "\n",
    "* **Filtered Signal (np.ndarray):** A NumPy array of the same shape as the input, containing the cleaned ECG signal with artifacts removed.\n",
    "* **Frequency Response (Optional):** Magnitude response data in dB for visualizing filter performance.\n",
    "* **Metadata:** String representations (`__repr__`) of the filter objects for logging experiment configurations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda7510e",
   "metadata": {},
   "source": [
    "### Step 1.2: QualityAssessor: SNR, flatlines, saturation\n",
    "\n",
    "**Script**: [src/preprocessing/quality.py](../src/preprocessing/quality.py)\n",
    "\n",
    "This module provides a comprehensive framework for evaluating the clinical and technical quality of ECG signals. It implements a multi-metric approach to detect common signal acquisition issues such as **electrode disconnection (flatlines)**, **amplifier saturation (clipping)**, **baseline instability**, and **excessive noise**. By aggregating these metrics into a weighted **Quality Score**, the module allows automated pipelines to decide whether a signal is reliable enough for deep learning inference or if it should be rejected to avoid false positives.\n",
    "\n",
    "\n",
    "#### Input\n",
    "\n",
    "The module's main components (`QualityAssessor`, `assess_quality`) take the following inputs:\n",
    "\n",
    "* **`signal` (np.ndarray):** The ECG data to be evaluated. It supports **1D arrays** (single lead) or **2D arrays** (multi-lead, e.g., 12-lead ECGs).\n",
    "* **`fs` (int):** The sampling frequency in Hz, used to define temporal windows for flatline detection and frequency bands for SNR calculation.\n",
    "* **`config` (QualityConfig, optional):** A custom configuration object to override default thresholds for:\n",
    "* **SNR limits** (default min: 5.0 dB).\n",
    "* **Flatline duration** (default min: 0.1s).\n",
    "* **Amplitude ranges** (default: 0.1mV to 5.0mV).\n",
    "\n",
    "#### Output\n",
    "\n",
    "The module returns a **`QualityMetrics`** object containing:\n",
    "\n",
    "* **Numerical Metrics:** Specific values for `snr_db`, `flatline_ratio`, `saturation_ratio`, `baseline_drift`, and `high_freq_noise`.\n",
    "* **`quality_score` (float):** A normalized value from **0.0 to 1.0** representing overall signal integrity.\n",
    "* **`quality_level` (Enum):** A categorical label: `EXCELLENT`, `GOOD`, `ACCEPTABLE`, `POOR`, or `UNUSABLE`.\n",
    "* **`is_usable` (bool):** A boolean flag indicating if the signal meets the minimum quality threshold for analysis.\n",
    "* **`issues` (List[str]):** A detailed list of detected problems (e.g., \"Lead 0: Low SNR\", \"Signal saturation\").\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531b84b1",
   "metadata": {},
   "source": [
    "### Step 1.3: BeatSegmenter: Pan-Tompkins R-detection\n",
    "\n",
    "**Script**: [src/preprocessing/segmentation.py](../src/preprocessing/segmentation.py)\n",
    "\n",
    "This module is the core engine for signal parsing within the **Kepler-ECG** project. It implements the classic **Pan-Tompkins algorithm** to detect R-peaks (the most prominent spikes in an ECG) with high precision. Beyond simple detection, it automatically segments the continuous cardiac signal into individual heartbeats, aligns them, and computes vital clinical metrics such as Heart Rate (BPM) and RR intervals. This segmentation is a fundamental prerequisite for training deep learning models on heartbeat-level morphologies.\n",
    "\n",
    "#### Input\n",
    "\n",
    "The primary classes (`PanTompkinsDetector`, `BeatSegmenter`) and the `segment_beats` function accept the following:\n",
    "\n",
    "* **`signal` (np.ndarray):** The raw or filtered ECG data. It supports **1D arrays** (single lead) or **2D arrays** (multi-lead).\n",
    "* **`fs` (int):** The sampling frequency in Hz (essential for time-to-sample conversions).\n",
    "* **`config` (SegmentationConfig, optional):** A configuration object to tune the algorithm, including:\n",
    "* **Bandpass limits:** Default 5.0 Hz to 15.0 Hz to isolate QRS energy.\n",
    "* **Search windows:** Time before (0.2s) and after (0.4s) the R-peak for beat extraction.\n",
    "* **Physiological limits:** Min/Max RR intervals to filter out noise that mimics heartbeats.\n",
    "\n",
    "#### Output\n",
    "\n",
    "The module returns a **`SegmentationResult`** object containing:\n",
    "\n",
    "* **`r_peaks` (np.ndarray):** The precise indices of all detected heartbeats in the signal.\n",
    "* **`beats` (np.ndarray):** A 2D matrix of shape `(n_beats, beat_length)` containing the extracted and aligned heartbeat waveforms.\n",
    "* **`heart_rate_bpm` (float):** The calculated mean heart rate.\n",
    "* **`detection_confidence` (float):** A score from **0.0 to 1.0** indicating the reliability of the detection based on rhythm regularity and signal consistency.\n",
    "* **`beat_template` (np.ndarray):** An \"average\" heartbeat calculated from all valid segments, useful for noise reduction.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e01998c",
   "metadata": {},
   "source": [
    "### Step 1.4: HRVPreprocessor: rimozione ectopici, interpolazione\n",
    "\n",
    "**Script**: [src/preprocessing/hrv_preprocessing.py](../src/preprocessing/hrv_preprocessing.py)\n",
    "\n",
    "This module is dedicated to the specific preprocessing required for **Heart Rate Variability (HRV)** analysis. Its primary purpose is to transform a sequence of raw RR intervals into a clean, uniform time series ready for time and frequency domain analysis. The module implements advanced algorithms for the identification of **ectopic beats** (irregular rhythms) and artifacts, utilizing clinical standards such as the Malik or Kamath criteria. Additionally, it includes interpolation functionality to handle missing data and ensure a constant sampling rate, which is essential for accurate spectral analysis of cardiac variability.\n",
    "\n",
    "#### Input\n",
    "\n",
    "The primary components of the module, such as the `HRVPreprocessor` class and the `preprocess_hrv` function, require the following inputs:\n",
    "\n",
    "* **`rr_intervals` (np.ndarray)**: An array containing RR intervals expressed in milliseconds.\n",
    "* **`config` (HRVConfig, optional)**: A configuration object used to customize the processing pipeline. This includes:\n",
    "* **Ectopic detection method**: Options include `MALIK`, `KAMATH`, `KARLSSON`, or `ACAR`.\n",
    "* **Detection threshold**: A numerical parameter to adjust the sensitivity of the artifact identification algorithm.\n",
    "* **Resampling frequency**: The target frequency (e.g., 4 Hz) for the interpolation of the time series.\n",
    "\n",
    "\n",
    "\n",
    "#### üì§ Output\n",
    "\n",
    "The module returns an **`HRVData`** object containing the following elements:\n",
    "\n",
    "* **`rr_clean` (np.ndarray)**: The series of RR intervals after the removal or correction of identified ectopic beats.\n",
    "* **`rr_interpolated` (np.ndarray)**: An equidistantly sampled time series, suitable for frequency-based analysis such as Fast Fourier Transform (FFT).\n",
    "* **`ectopic_mask` (np.ndarray)**: A boolean array indicating which original beats were flagged as anomalies or artifacts.\n",
    "* **`metrics` (Dict)**: Basic descriptive statistics, such as the percentage of detected ectopic beats within the signal.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125d58ab",
   "metadata": {},
   "source": [
    "### Step 1.5: PreprocessingPipeline\n",
    "\n",
    "**Script**: [src/preprocessing/pipeline.py](../src/preprocessing/pipeline.py)\n",
    "\n",
    "This module serves as the central orchestration engine for the **Kepler-ECG** project, integrating all individual preprocessing components into a unified and automated workflow. It sequentially executes signal filtering (baseline and noise removal), quality assessment, R-peak detection, beat segmentation, and HRV analysis. Designed for high-performance research, the pipeline features **parallel processing** to handle large datasets efficiently and a **caching system** (using SHA-256 hashing) to avoid redundant computations. It provides a robust interface for transforming raw physiological data into structured, analysis-ready objects.\n",
    "\n",
    "#### Input\n",
    "\n",
    "The `PreprocessingPipeline` class and its convenience functions (like `process_ecg_batch`) accept the following:\n",
    "\n",
    "* **`signal` (np.ndarray)**: The raw ECG waveform, supporting both single-lead (1D) and multi-lead (2D) configurations.\n",
    "* **`fs` (int)**: The sampling frequency in Hz, required by all internal stages for accurate time-domain processing.\n",
    "* **`PipelineConfig` (optional)**: A comprehensive configuration object that allows fine-tuning of:\n",
    "* **Filter Settings**: Cutoff frequencies for high-pass, low-pass, and notch filters.\n",
    "* **Execution Settings**: `n_jobs` for parallel execution and `enable_cache` to toggle disk-based storage of results.\n",
    "* **Quality & Segmentation**: Thresholds for signal rejection and beat extraction parameters.\n",
    "\n",
    "#### Output\n",
    "\n",
    "The pipeline returns a **`ProcessedECG`** object (or a list of objects for batches), which contains:\n",
    "\n",
    "* **`filtered_signal`**: The cleaned ECG data.\n",
    "* **`quality_metrics`**: The results from the quality assessment stage (score, level, and usability).\n",
    "* **`segmentation`**: R-peak indices, extracted heartbeat segments, and heart rate statistics.\n",
    "* **`hrv_data`**: Cleaned and interpolated RR intervals for variability analysis.\n",
    "* **`metadata`**: A dictionary containing processing time, timestamps, and the unique hash identifier for the operation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadd2228",
   "metadata": {},
   "source": [
    "### Step 1.6: Processare intero dataset\n",
    "\n",
    "**Script**: [scripts/process_dataset.py](../scripts/process_dataset.py)\n",
    "\n",
    "This module is a high-level CLI (Command Line Interface) script designed to automate the processing of large-scale ECG datasets. It acts as a bridge between raw data storage and the **Kepler-ECG Preprocessing Pipeline**, featuring an auto-detection system that recognizes various clinical data formats such as **WFDB** (.dat/.hea), **MAT** (.mat), and **NumPy** (.npy). The script is optimized for research workflows, allowing for targeted sampling rate filtering and batch processing to handle thousands of records while providing detailed logging and progress reporting.\n",
    "\n",
    "#### Input (Command Line Arguments)\n",
    "\n",
    "The script requires several parameters to define the data workflow:\n",
    "\n",
    "* **`--data_dir` (Required)**: The path to the directory containing the raw ECG dataset.\n",
    "* **`--output_dir` (Required)**: The path where the processed results, logs, and metadata will be saved.\n",
    "* **`--sampling_rate` (Optional)**: A filter to process only files with a specific sampling frequency (e.g., 500 Hz for PTB-XL).\n",
    "* **`--n_samples` (Optional)**: Limits the processing to a specific number of records (useful for testing).\n",
    "* **`--batch_size` (Default: 100)**: Determines how many records are processed before reporting progress.\n",
    "\n",
    "#### Output\n",
    "\n",
    "The execution produces a structured output in the destination folder:\n",
    "\n",
    "* **Processed Data**: Individual preprocessed objects (typically saved as `.pkl` or `.npy`) containing filtered signals and extracted features.\n",
    "* **`processing_summary.json`**: A global metadata file summarizing the execution time, number of files processed, and any errors encountered.\n",
    "* **Quality Statistics**: A detailed breakdown in the logs showing the distribution of signal quality levels (e.g., % of Excellent vs. Unusable signals).\n",
    "* **Execution Logs**: A professional log file tracking every step of the batch operation.\n",
    "\n",
    "#### Running the script for different datasets\n",
    "\n",
    "```bash\n",
    "# 1. Help\n",
    "python scripts/process_dataset.py --help\n",
    "\n",
    "# 2. Process ptb-xl\n",
    "python scripts/process_dataset.py --dataset ptb-xl --sampling_rate 500\n",
    "```\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c80f81",
   "metadata": {},
   "source": [
    "## üìä FASE 2: Features Extraction\n",
    "\n",
    "### Objective\n",
    "\n",
    "Extract 100+ features for ECG: morphological, spectral, wavelet, compressibility.\n",
    "\n",
    "### Step 2.1: MorphologicalExtractor\n",
    "\n",
    "**Script**: [src/features/morphological.py](../src/features/morphological.py)\n",
    "\n",
    "This module is designed to extract detailed clinical and morphological features from ECG beat templates. It identifies key characteristic points of the cardiac cycle, such as the onset, peak, and offset of P, Q, R, S, and T waves. By calculating specific wave durations, amplitudes, and intervals (including corrected QT intervals using Bazett, Fridericia, and Framingham formulas), the module provides a structured digital representation of the heartbeat's shape. These features are essential for both traditional clinical diagnostics and as structured inputs for deep learning classifiers.\n",
    "\n",
    "#### Input\n",
    "\n",
    "The core components, specifically the `MorphologicalExtractor` and the `WavePoints` data structure, process the following inputs:\n",
    "\n",
    "* **`beat_template` (np.ndarray)**: A 1D array representing a single, representative heartbeat (typically the average or median beat) centered on the R-peak.\n",
    "* **`fs` (int)**: The sampling frequency in Hz, used to convert sample indices into clinical time measurements in milliseconds.\n",
    "* **`rr_mean_ms` (optional float)**: The mean RR interval in milliseconds, required for calculating corrected QT (QTc) intervals.\n",
    "\n",
    "#### Output\n",
    "\n",
    "The module returns a comprehensive dictionary or data structure containing:\n",
    "\n",
    "* **Wave Amplitudes**: Peak values for P, Q, R, S, and T waves, along with ST-segment levels.\n",
    "* **Temporal Intervals**: Precise durations in milliseconds for the P-wave, QRS complex, T-wave, PR interval, and QT interval.\n",
    "* **Corrected Metrics**: QTc values calculated via various medical formulas (Bazett, Fridericia, Framingham).\n",
    "* **Morphological Ratios**: Calculated values such as the R/S ratio and T/R ratio, as well as areas under the QRS and T curves.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941d7547",
   "metadata": {},
   "source": [
    "### Step 2.2: SpectralAnalyzer (HRV)\n",
    "\n",
    "**Script**: [src/features/spectral.py](../src/features/spectral.py)\n",
    "\n",
    "This module focuses on the frequency-domain analysis of Heart Rate Variability (HRV). It transforms clean RR interval sequences into power spectral density (PSD) estimates to quantify the autonomic nervous system's influence on the heart. The module implements both the **Welch periodogram** for uniformly resampled series and the **Lomb-Scargle periodogram** for non-uniform data. It is designed to extract standard clinical frequency bands (VLF, LF, HF) and calculate the LF/HF ratio, which serves as an indicator of the sympathetic-vagal balance.\n",
    "\n",
    "#### Input\n",
    "\n",
    "The primary class `SpectralAnalyzer` and its associated functions take the following inputs:\n",
    "\n",
    "* **`rr_intervals` (np.ndarray)**: An array of RR intervals in milliseconds.\n",
    "* **`fs_interpolation` (float, default=4.0)**: The frequency in Hz used to resample the RR series for Welch analysis.\n",
    "* **`method` (Literal['welch', 'lomb-scargle'])**: The spectral estimation technique to be applied.\n",
    "* **`bands` (FrequencyBands, optional)**: A configuration object defining the frequency limits for:\n",
    "* **VLF (Very Low Frequency)**: 0.003 - 0.04 Hz.\n",
    "* **LF (Low Frequency)**: 0.04 - 0.15 Hz.\n",
    "* **HF (High Frequency)**: 0.15 - 0.4 Hz.\n",
    "\n",
    "#### Output\n",
    "\n",
    "The module returns a **`SpectralMetrics`** object (or a dictionary) containing:\n",
    "\n",
    "* **Absolute Power**: Total power and power for each specific band (VLF, LF, HF) measured in .\n",
    "* **Normalized Power**: LF and HF components expressed in normalized units (n.u.).\n",
    "* **LF/HF Ratio**: A numerical value representing the balance between the sympathetic and parasympathetic systems.\n",
    "* **Peak Frequencies**: The specific frequency values (in Hz) where the maximum power occurs within each band.\n",
    "* **PSD Data**: The frequency vector and power spectral density values for visualization (e.g., plotting the periodogram).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2949d4c",
   "metadata": {},
   "source": [
    "### Step 2.3: WaveletExtractor\n",
    "\n",
    "**Script**: [src/features/wavelet.py](../src/features/wavelet.py)\n",
    "\n",
    "This module implements feature extraction based on the **Discrete Wavelet Transform (DWT)**, a fundamental technique for the time-frequency analysis of ECG signals. By decomposing the signal into different scales (levels), the module isolates fine details like spikes and noise at lower scales, the QRS complex at middle scales, and P/T waves or baseline wander at higher scales. It is particularly effective at capturing non-stationary morphological variations that traditional methods might miss, providing a set of mathematical descriptors (energy, entropy, and statistical parameters) ideal for Deep Learning models.\n",
    "\n",
    "#### Input\n",
    "\n",
    "The `WaveletExtractor` class and its primary functions accept the following:\n",
    "\n",
    "* **`signal` (np.ndarray)**: The ECG signal (either a segmented beat or a continuous strip) to be decomposed.\n",
    "* **`WaveletConfig` (optional)**: A configuration object used to define:\n",
    "* **`wavelet`**: The type of mother wavelet (defaults to `'db4'` ‚Äî Daubechies 4, which is highly effective for ECG morphology).\n",
    "* **`max_level`**: The number of decomposition levels (if `None`, it is automatically calculated based on signal length).\n",
    "* **`mode`**: The signal extension mode (e.g., `'symmetric'`).\n",
    "\n",
    "#### Output\n",
    "\n",
    "The module returns a dictionary or a **`WaveletFeatures`** object containing:\n",
    "\n",
    "* **`energy_per_level`**: The distribution of signal energy across the various approximation and detail levels.\n",
    "* **`wavelet_entropy`**: A measure of signal complexity or disorder based on the wavelet energy distribution.\n",
    "* **`coefficient_statistics`**: A set of statistical parameters (mean, standard deviation, skewness, and kurtosis) calculated for each decomposition level.\n",
    "* **`coefficients`**: The raw wavelet coefficients (optional), useful for signal reconstruction or custom filtering.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616ba8b3",
   "metadata": {},
   "source": [
    "### Step 2.4: CompressibilityCalculator ‚≠ê CORE\n",
    "\n",
    "**Script**: [src/features/compressor.py](../src/features/compressor.py)\n",
    "\n",
    "This module implements advanced techniques to measure the **algorithmic complexity** and regularity of ECG signals and RR time series. It utilizes compressibility as a proxy for Kolmogorov complexity, based on the premise that more regular and predictable signals are more easily compressed. The module includes classic entropy algorithms (Sample, Approximate, Permutation) and Lempel-Ziv complexity metrics, providing a non-linear characterization of the signal that complements standard morphological and spectral analyses.\n",
    "\n",
    "#### Input\n",
    "\n",
    "The primary components, such as the `CompressibilityCalculator` class, accept the following parameters:\n",
    "\n",
    "* **`signal` (np.ndarray)**: The filtered ECG signal or the RR interval series.\n",
    "* **`CompressibilityConfig` (optional)**: A configuration object to tune the algorithm parameters:\n",
    "* **`entropy_m`**: Embedding dimension for entropy calculation (default: 2).\n",
    "* **`entropy_r_factor`**: Tolerance factor (default: 0.2 * standard deviation of the signal).\n",
    "* **`perm_order` & `perm_delay**`: Specific parameters for Permutation Entropy.\n",
    "\n",
    "#### Output\n",
    "\n",
    "The module returns a dictionary of numerical features (**`Dict[str, float]`**) including:\n",
    "\n",
    "* **Compression Ratios**: Ratios calculated using `gzip`, `bzip2`, and `lzma` algorithms.\n",
    "* **Entropy Metrics**: Values for *Sample Entropy*, *Approximate Entropy*, *Permutation Entropy*, and *Shannon Entropy*.\n",
    "* **Complexity Measures**: Lempel-Ziv complexity index and Kolmogorov complexity estimates.\n",
    "* **Hjorth Parameters**: Statistical time-domain metrics for activity, mobility, and complexity.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae618604",
   "metadata": {},
   "source": [
    "### Step 2.5: Unified Label Schema Module\n",
    "\n",
    "**Script**: [src/core/label_schema.py](../src/core/label_schema.py)\n",
    "\n",
    "The **Unified Label Schema** is the taxonomic heart of the **Kepler-ECG** multi-dataset framework. It solves the critical challenge of clinical \"interoperability\" by mapping disparate diagnostic codes from various international databases (like SCP-ECG codes from PTB-XL or SNOMED-CT from the Chapman dataset) into a standardized, hierarchical structure. By organizing cardiac conditions into five major superclasses‚Äî**NORM, MI, STTC, CD, and HYP**‚Äîit allows models trained on one dataset to be validated against another, ensuring a consistent ground truth for both Deep Learning and Symbolic Regression tasks.\n",
    "\n",
    "#### Input\n",
    "\n",
    "The `LabelMapper` class and its primary methods (e.g., `map_scp_codes`, `map_snomed`) accept:\n",
    "\n",
    "* **`codes` (Dict or List)**: The raw diagnostic output from a dataset. This could be a dictionary of SCP-ECG codes with confidence scores or a list of SNOMED-CT numerical strings.\n",
    "* **`dataset_name` (str)**: Used by the mapper to select the appropriate internal translation table (e.g., `'ptb-xl'`, `'chapman'`, `'georgia'`).\n",
    "* **`threshold` (float, default=0.0)**: A confidence filter to ignore diagnostic labels that fall below a certain probability in the original metadata.\n",
    "\n",
    "#### Output\n",
    "\n",
    "The module returns structured **`Label`** objects and vectors:\n",
    "\n",
    "* **`mapped_labels` (List[Label])**: A list of objects containing the `Superclass`, `Subclass`, and the original `specific_code`.\n",
    "* **`superclass_vector` (np.ndarray)**: A binary multi-hot encoded vector (length 5) representing the presence of NORM, MI, STTC, CD, and HYP.\n",
    "* **`primary_superclass` (Enum)**: The dominant clinical category for the record, determined by the highest confidence score or clinical priority.\n",
    "* **`original_codes` (List[str])**: A clean list of the original strings used for the mapping, preserved for auditing and clinical verification.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd30d311",
   "metadata": {},
   "source": [
    "### Step 2.6: Feature Pipeline\n",
    "\n",
    "**Script**: [src/features/feature_pipeline.py](../src/features/feature_pipeline.py)\n",
    "\n",
    "The **Feature Pipeline** is the high-level orchestrator designed to consolidate all extraction methodologies within the **Kepler-ECG** ecosystem. It acts as a unified interface that coordinates the simultaneous extraction of morphological, interval-based, spectral (HRV), and wavelet-based features. By transforming preprocessed ECG signals and detected R-peaks into a comprehensive numerical feature vector, this module prepares the data for statistical analysis or as input for machine learning models. It supports batch processing with multi-threading and includes robust error handling to ensure pipeline stability during large-scale dataset operations.\n",
    "\n",
    "#### Input\n",
    "\n",
    "The `FeaturePipeline` class and its main execution methods (e.g., `extract_all_features`) require:\n",
    "\n",
    "* **`signal` (np.ndarray)**: The preprocessed/filtered ECG waveform.\n",
    "* **`r_peaks` (np.ndarray)**: The indices of detected R-peaks (obtained from the `Segmentation` module).\n",
    "* **`sampling_rate` (int)**: The frequency (Hz) used for time-to-sample conversions.\n",
    "* **`FeatureConfig` (optional)**: A structured configuration object to toggle specific extractors:\n",
    "* `extract_morphological`: Boolean to enable/disable wave shape analysis.\n",
    "* `extract_spectral`: Boolean for HRV frequency domain analysis.\n",
    "* `extract_wavelet`: Boolean for multi-scale decomposition.\n",
    "* `n_jobs`: Number of threads for parallel batch extraction.\n",
    "\n",
    "#### Output\n",
    "\n",
    "The pipeline returns a **`FeatureVector`** (or a list of vectors for batch mode) containing:\n",
    "\n",
    "* **`morphological_features`**: A set of amplitudes and areas (P, Q, R, S, T waves).\n",
    "* **`interval_features`**: Clinical durations (PR, QRS, QT) and corrected QTc values.\n",
    "* **`spectral_features`**: Frequency band powers (VLF, LF, HF) and the LF/HF ratio.\n",
    "* **`wavelet_features`**: Energy and entropy metrics across multiple decomposition levels.\n",
    "* **`processing_metadata`**: Information regarding execution time and potential warnings (e.g., if a specific feature couldn't be calculated due to signal length).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7832bc6a",
   "metadata": {},
   "source": [
    "### Step 2.7: FeaturePipeline completa\n",
    "\n",
    "**Script**: [scripts/extract_features_dataset.py](../scripts/extract_features_dataset.py)\n",
    "\n",
    "This script serves as the final operational utility in the **Kepler-ECG** preprocessing suite. It is a high-level command-line tool designed to perform \"Phase 2\" of data preparation: extracting advanced multidimensional features from a previously processed dataset. It synchronizes raw signal data with metadata, applying morphological, spectral, wavelet, and compressibility extractors simultaneously. Additionally, it integrates the **Diagnosis Mapper** to align clinical labels, resulting in a comprehensive, \"ML-ready\" CSV file that can be fed directly into deep learning models or statistical analysis tools.\n",
    "\n",
    "#### Input (Command Line Arguments)\n",
    "\n",
    "The script requires specific paths and flags to coordinate the extraction process:\n",
    "\n",
    "* **`--phase1` (Required)**: Path to the CSV file generated during Phase 1 (e.g., `ptb-xl_features.csv`), which contains file pointers and basic metadata.\n",
    "* **`--data-dir` (Required)**: The root directory where the original raw ECG files are stored.\n",
    "* **`--output` (Optional)**: Specific path for the output CSV. If omitted, it appends `_features_extracted` to the input filename.\n",
    "* **`--lead` (Default: 0)**: Specifies which ECG lead to extract features from (essential for multi-lead datasets).\n",
    "* **Feature Toggles**: Optional flags to disable specific extractions (e.g., `--no-wavelet`, `--no-compressibility`, `--no-diagnosis`).\n",
    "* **Processing Limits**: `--n_samples` and `--start` to allow for partial dataset processing or resume capabilities.\n",
    "\n",
    "#### Output\n",
    "\n",
    "The script generates a final structured dataset and execution logs:\n",
    "\n",
    "* **`*ptb-xl_features_extracted.csv`**: A dense tabular file where each row represents an ECG record and columns contain the full suite of extracted features (Amplitudes, Intervals, QTc, Spectral powers, Wavelet coefficients, Entropy, and diagnostic classes).\n",
    "* **Progress Telemetry**: A real-time progress bar (via `tqdm`) showing the processing rate (samples per second) and estimated time of completion.\n",
    "* **Statistical Summary**: A final console output detailing the number of successful extractions versus failures (due to signal quality or missing files).\n",
    "\n",
    "#### Running the script for different datasets\n",
    "\n",
    "```bash\n",
    "# Extract features for PTB-XL\n",
    "python scripts/extract_features_dataset.py --dataset ptb-xl\n",
    "```\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5ababf",
   "metadata": {},
   "source": [
    "## üîç FASE 3: Discovery & Analysis\n",
    "\n",
    "### Objective\n",
    "Statistical validation, visualizations, dataset preparation for SR.\n",
    "\n",
    "### Step 3.1: Statistical Analysis\n",
    "\n",
    "**Script**: [src/analysis/statistical.py](../src/analysis/statistical.py)\n",
    "\n",
    "The **Statistical Analysis Module** is the core component of \"Phase 3\" in the **Kepler-ECG** project. It provides a robust framework for validating the clinical relevance of extracted features across different diagnostic groups. The module implements a suite of parametric and non-parametric tests (ANOVA, Kruskal-Wallis, T-tests) to determine if features like HRV spectral power or wavelet entropy vary significantly between healthy subjects and patients with cardiac pathologies. It also calculates effect sizes (Cohen's ) and performs correlation analysis to identify the most discriminative biomarkers for subsequent Deep Learning training.\n",
    "\n",
    "#### Input\n",
    "\n",
    "The main analytical classes (`StatisticalAnalyzer`) and their methods accept:\n",
    "\n",
    "* **`df` (pd.DataFrame)**: The structured feature matrix (typically the `*_features_extracted.csv` file) containing both numerical features and categorical diagnostic labels.\n",
    "* **`feature` (str)**: The name of the specific column to analyze (e.g., `'hf_power'`, `'qrs_duration'`).\n",
    "* **`category_col` (str)**: The column used for grouping data (e.g., `'diagnostic_superclass'`).\n",
    "* **`config` (optional)**: Parameters to define the significance level (default ) and minimum group size requirements.\n",
    "\n",
    "#### Output\n",
    "\n",
    "The module returns structured result objects (**`ANOVAResult`**, **`PairwiseComparison`**) containing:\n",
    "\n",
    "* **Global Statistics**: F-statistic or H-statistic and their corresponding p-values to check for overall group differences.\n",
    "* **Significance Flags**: Boolean indicators (`is_significant`) and visual markers (e.g., `***` for ).\n",
    "* **Pairwise Comparisons**: Detailed results from post-hoc tests (e.g., Mann-Whitney U) showing exactly which diagnostic classes differ from each other.\n",
    "* **Effect Sizes**: Cohen's  or similar metrics to quantify the magnitude of the difference between groups.\n",
    "* **Descriptive Stats**: Group-specific means, standard deviations, and sample counts for reporting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ab1ea7",
   "metadata": {},
   "source": [
    "### Step 3.2: Research Visualization\n",
    "\n",
    "**Script**: [src/analysis/visualization.py](../src/analysis/visualization.py)\n",
    "\n",
    "The **Visualization Module** is the graphical engine for the final phase of the **Kepler-ECG** project. It is designed to produce publication-quality charts and exploratory data analysis (EDA) plots that reveal the underlying patterns in cardiac data. The module focuses on the relationship between mathematical features (Morphological, Spectral, Wavelet, and Compressibility) and clinical diagnoses. It provides advanced tools for dimensionality reduction, correlation mapping, and group-wise distribution analysis, ensuring that the results of the Deep Learning and Statistical phases are interpretable and visually compelling.\n",
    "\n",
    "\n",
    "#### Input\n",
    "\n",
    "The `ECGVisualizer` class and its plotting methods take the following inputs:\n",
    "\n",
    "* **`df` (pd.DataFrame)**: The processed feature matrix containing numerical extractions and categorical labels (NORM, MI, STTC, CD, HYP).\n",
    "* **`features` (List[str])**: A list of specific feature names to be visualized (e.g., `['samp_en', 'hf_power', 'qrs_duration']`).\n",
    "* **`target_col` (str)**: The categorical column used for grouping and coloring the data (default: `diagnostic_superclass`).\n",
    "* **`VisualizationConfig` (optional)**: Parameters to customize plot aesthetics, such as color palettes (e.g., the predefined `DIAGNOSIS_COLORS`), figure sizes, and DPI settings for high-resolution export.\n",
    "\n",
    "#### Output\n",
    "\n",
    "The module generates high-fidelity visual outputs and saved files:\n",
    "\n",
    "* **Statistical Plots**: Box plots and violin plots showing the distribution and variance of features across different pathologies.\n",
    "* **Correlation Heatmaps**: Matrices illustrating the redundancy or independence between different ECG descriptors.\n",
    "* **Manifold Projections**: 2D or 3D scatter plots using **t-SNE** or **PCA** to visualize how well the different diagnostic classes cluster in the feature space.\n",
    "* **Exported Files**: High-resolution images saved in multiple formats (e.g., `.png`, `.pdf`, `.svg`) in a structured `reports/` directory.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4c7262",
   "metadata": {},
   "source": [
    "\n",
    "### Step 3.3: Feature Selection\n",
    "\n",
    "**Script**: [src/analysis/feature_selection.py](../src/analysis/feature_selection.py)\n",
    "\n",
    "The **Feature Selection Module** is a critical analytical component of Phase 3, designed to identify the most discriminative physiological biomarkers from the high-dimensional feature set generated in Phase 2. It implements a hybrid selection strategy combining univariate statistical tests (ANOVA F-score, Mutual Information) with multivariate machine learning models (Random Forest, Gradient Boosting). By performing redundancy analysis and calculating importance rankings, the module filters out noisy or highly correlated features, ensuring that subsequent Symbolic Regression and Deep Learning models are efficient, interpretable, and less prone to overfitting.\n",
    "\n",
    "#### Input\n",
    "\n",
    "The primary class `FeatureSelector` and its selection methods (e.g., `calculate_importance`, `select_best_features`) take the following inputs:\n",
    "\n",
    "* **`df` (pd.DataFrame)**: The input feature matrix containing all extracted ECG metrics and clinical labels.\n",
    "* **`target_col` (str)**: The objective variable for selection (e.g., `'diagnostic_superclass'` for classification or `'age'` for regression).\n",
    "* **`n_features` (int, default=10)**: The target number of top-performing features to retain.\n",
    "* **`max_correlation` (float, default=0.85)**: The threshold for redundancy filtering; if two high-ranking features are more correlated than this value, the lower-ranked one is discarded.\n",
    "* **`method` (str)**: The algorithm to use for importance calculation (e.g., `'random_forest'`, `'mutual_info'`, or `'gradient_boosting'`).\n",
    "\n",
    "#### Output\n",
    "\n",
    "The module returns a **`SelectionSummary`** (or a detailed dictionary) containing:\n",
    "\n",
    "* **`suggested_features` (List[str])**: The final list of selected feature names, optimized for predictive power and low redundancy.\n",
    "* **`importance_scores` (pd.DataFrame)**: A ranked table showing the raw scores (F-score, Gini importance, or MI) and the final rank for every feature analyzed.\n",
    "* **`group_distribution` (Dict)**: A breakdown showing how many features were selected from each category (e.g., 3 Morphological, 4 Spectral, 3 Wavelet).\n",
    "* **`redundancy_report`**: Information on which features were removed due to high multicollinearity.\n",
    "* **`baseline_comparison`**: Cross-validation scores showing the performance of a model using only the selected features versus the full feature set.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85fa184",
   "metadata": {},
   "source": [
    "### Step 3.4: Symbolic Regression Preparation\n",
    "\n",
    "**Script**: [src/analysis/sr_preparation.py](../src/analysis/sr_preparation.py)\n",
    "\n",
    "This module is a specialized data engineering component designed to bridge the gap between extracted ECG features and **Symbolic Regression (SR)** tools like PySR. Its primary purpose is to transform complex feature matrices into highly optimized, clean, and normalized datasets. It supports different machine learning tasks, including binary classification (e.g., Normal vs. Pathology), multi-class classification, and regression (e.g., predicting cardiac age). By automating data cleaning, feature scaling, and train-test splitting, it ensures that the mathematical formulas discovered by SR are both statistically sound and interpretable.\n",
    "\n",
    "\n",
    "#### Input\n",
    "\n",
    "The `SRDataPreparer` class and its associated methods take the following inputs:\n",
    "\n",
    "* **`df` (pd.DataFrame)**: The comprehensive feature matrix generated in earlier phases (containing morphological, spectral, and wavelet features).\n",
    "* **`target_col` (str)**: The dependent variable to be predicted (e.g., a specific diagnostic class or a numerical value).\n",
    "* **`feature_list` (List[str], optional)**: A specific subset of features to include in the analysis to reduce dimensionality.\n",
    "* **`PreparationConfig` (optional)**: Configuration for data processing, including:\n",
    "* **Normalization Method**: Choice between `StandardScaler` (Z-score) or `MinMaxScaler`.\n",
    "* **Task Type**: Defines the objective as `binary`, `multiclass`, or `regression`.\n",
    "* **Test Split Ratio**: The percentage of data reserved for validation (e.g., 0.2).\n",
    "\n",
    "#### Output\n",
    "\n",
    "The module returns an **`SRDataset`** object, which contains:\n",
    "\n",
    "* **`X` (np.ndarray)**: The cleaned and scaled feature matrix ready for the regressor.\n",
    "* **`y` (np.ndarray)**: The encoded target vector (numeric labels for classification or continuous values for regression).\n",
    "* **`feature_names`**: A list of strings corresponding to the columns in `X`, essential for the symbolic regressor to name variables in the discovered formulas (e.g.,  becomes `hf_power`).\n",
    "* **`baseline_metrics`**: A dictionary containing performance scores from a baseline model (Logistic or Linear Regression) to provide a benchmark for the Symbolic Regression results.\n",
    "* **Metadata**: Information regarding the scaling parameters used, enabling the reversal of normalization for interpretation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b4b2c8",
   "metadata": {},
   "source": [
    "### Step 3.5: Phase 3 pipeline\n",
    "\n",
    "**Script**: [src/analysis/pipeline.py](../src/analysis/pipeline.py)\n",
    "\n",
    "The **Phase 3 Pipeline Orchestrator** is the final management layer of the **Kepler-ECG** project. Its role is to automate the entire analytical workflow following feature extraction. It sequentially triggers statistical validation, generates a full suite of research visualizations, performs intelligent feature selection, and exports data specifically formatted for **Symbolic Regression**. This module ensures that the transition from raw physiological features to interpretable mathematical models is consistent, reproducible, and ready for clinical reporting.\n",
    "\n",
    "#### Input\n",
    "\n",
    "The `Phase3Pipeline` class and its execution command accept:\n",
    "\n",
    "* **`data_path` (str)**: The path to the comprehensive feature CSV file (e.g., `ptb-xl_features_extracted.csv`) generated in Phase 2.\n",
    "* **`output_dir` (str)**: The destination folder where all reports, plots, and datasets will be stored.\n",
    "* **`PipelineConfig` (optional)**: A configuration object to customize the execution:\n",
    "* **`sr_features_count`**: The number of top features to select for Symbolic Regression.\n",
    "* **`skip_viz`**: A boolean flag to bypass image generation for faster processing.\n",
    "* **`significance_level`**: The alpha threshold (e.g., 0.05) for statistical tests.\n",
    "\n",
    "#### Output\n",
    "\n",
    "The pipeline produces a structured directory of results, typically including:\n",
    "\n",
    "* **Statistical Reports**: JSON and text files summarizing ANOVA results and p-values for all biomarkers.\n",
    "* **Visualization Gallery**: A subfolder containing high-resolution PNG/PDF plots (Heatmaps, Box plots, t-SNE clusters).\n",
    "* **Selected Features List**: A filtered dataset containing only the most discriminative, non-redundant features.\n",
    "* **SR-Ready Datasets**: Multiple CSV files (e.g., `sr_binary_MI.csv`) formatted specifically for discovery of mathematical formulas.\n",
    "* **`pipeline_results.json`**: A master metadata file containing baseline performance metrics (e.g., Logistic Regression accuracy) and processing timestamps.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5ed4c4",
   "metadata": {},
   "source": [
    "### Step 3.6:  Multi-Dataset Analysis Orchestrator\n",
    "\n",
    "**Script**: [scripts/analyze_dataset.py](../scripts/analyze_dataset.py)\n",
    "\n",
    "The **Dataset Analysis Script** is the primary execution entry point for **Phase 3** of the Kepler-ECG project. It acts as a high-level manager that automates the transition from raw extracted features to scientific insights. Designed with multi-dataset support in mind, it orchestrates four critical sub-processes: statistical significance testing, publication-quality visualization, intelligent feature selection, and data formatting for Symbolic Regression. This script allows researchers to process individual datasets (like PTB-XL or Chapman) or execute a full-scale comparative analysis across all available ECG databases with a single command.\n",
    "\n",
    "#### Input (Command Line Arguments)\n",
    "\n",
    "The script is highly configurable via CLI flags to tailor the analysis depth:\n",
    "\n",
    "* **`--dataset` (str)**: The name of the dataset to analyze (e.g., `ptb-xl`, `chapman`, `georgia`).\n",
    "* **`--all` (flag)**: Automatically detects and analyzes every processed dataset found in the results directory.\n",
    "* **`--input` (Path)**: Allows manual specification of a custom CSV file if the standard directory structure isn't used.\n",
    "* **`--n_features` (int, default: 12)**: Sets the target number of optimized features to select for the subsequent Symbolic Regression phase.\n",
    "* **`--skip-viz` (flag)**: Bypasses the generation of plots to speed up execution in headless or server environments.\n",
    "\n",
    "#### Output\n",
    "\n",
    "The script generates a structured \"Analysis Package\" within the `results/[dataset_name]/` directory:\n",
    "\n",
    "* **`analysis_summary.json`**: A master metadata file containing records counts, processing timestamps, and high-level success metrics.\n",
    "* **Statistical & Selection Reports**: Detailed JSON files highlighting p-values, effect sizes, and the top-ranked physiological biomarkers.\n",
    "* **Visualization Gallery**: A folder of high-resolution plots (Heatmaps, Box plots, PCA clusters) if not skipped.\n",
    "* **SR-Ready Files**: Subsets of the data specifically cleaned and formatted for the `PySR` discovery engine in Phase 4.\n",
    "* **Console Telemetry**: A real-time log showing the progress of each analytical stage and a final \"Multi-Dataset Summary\" table if processing multiple sources.\n",
    "\n",
    "#### Running the pipeline for different datasets\n",
    "\n",
    "```bash\n",
    "# Lista dataset disponibili\n",
    "python scripts/analyze_dataset.py --list\n",
    "\n",
    "# Analizza PTB-XL\n",
    "python scripts/analyze_dataset.py --dataset ptb-xl\n",
    "\n",
    "# Analizza tutti (skip visualizzazioni per velocit√†)\n",
    "python scripts/analyze_dataset.py --all --skip-viz\n",
    "\n",
    "# Analizza Chapman con 20 features\n",
    "python scripts/analyze_dataset.py --dataset chapman --n-features 20\n",
    "```\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9679b319",
   "metadata": {},
   "source": [
    "## üß¨ FASE 4: Symbolic Regression Discovery, Wave Delineation & QTc Validation\n",
    "\n",
    "### Objective\n",
    "Discover formulas that can be interpreted with PySR, extract QT/RR from all ECGs, validate Kepler formulas.\n",
    "\n",
    "### Step 4.1: Setup PySR/Julia\n",
    "\n",
    "```bash\n",
    "pip install pysr\n",
    "python -c \"import pysr; pysr.install()\"  # Installa Julia backend\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4107de8b",
   "metadata": {},
   "source": [
    "### Step 4.2: Stream A - Symbolic Regression for Diagnostic Categories Binary Classification\n",
    "\n",
    "**Script**: [scripts/sr_classification.py](../scripts/sr_classification.py)\n",
    "\n",
    "The **Symbolic Regression Classification Script** is an advanced discovery tool within the Kepler-ECG framework. Its primary objective is to find interpretable mathematical formulas that distinguish between specific diagnostic categories (e.g., Healthy/NORM vs. Myocardial Infarction/MI). Leveraging the `PySR` library, it evolves equations that serve as transparent alternatives to \"black-box\" machine learning models. The script features high-performance configuration, including logistic loss functions specifically for classification, automatic threshold calibration using Youden's J statistic, and 5-fold cross-validation to ensure the discovered formulas are medically robust and generalizable.\n",
    "\n",
    "#### Input (Command Line Arguments)\n",
    "\n",
    "The script can either use a pre-prepared subset or generate one from a full feature file:\n",
    "\n",
    "* **`--input` (Path)**: Path to a specific CSV file already formatted for SR (e.g., `ptb-xl_norm_vs_hyp.csv`).\n",
    "* **`--dataset` (str)**: Name of the dataset to process (e.g., `ptb-xl`, `chapman`) if using the auto-preparation mode.\n",
    "* **`--positive-class` (str)**: The diagnostic category to detect (e.g., `MI`, `HYP`, `CD`).\n",
    "* **`--negative-class` (default: 'NORM')**: The reference category to compare against.\n",
    "* **`--all` (flag)**: If set, the script iterates through all available pathological classes in the dataset.\n",
    "* **`--iterations` (int, default: 100)**: Number of evolutionary cycles for the PySR engine.\n",
    "\n",
    "#### Output\n",
    "\n",
    "The script generates a results directory containing the mathematical and statistical findings:\n",
    "\n",
    "* **`sr_results.json`**: A master file containing the \"Best Equation\" (selected by the highest score/complexity ratio), its performance metrics, and the optimal classification threshold.\n",
    "* **`hall_of_fame.csv`**: A list of all candidate equations found during the search, ranked by their Pareto efficiency (accuracy vs. simplicity).\n",
    "* **Performance Benchmarks**:\n",
    "* **AUC (Area Under Curve)**: Comparing the SR formula against a baseline Logistic Regression.\n",
    "* **Sensitivity & Specificity**: Calculated at the optimal decision point.\n",
    "\n",
    "\n",
    "* **Visualizations**:\n",
    "* **ROC & Precision-Recall Curves**: Graphical performance comparison.\n",
    "* **Pareto Front Plot**: Shows the trade-off between equation length and error.\n",
    "\n",
    "#### Running\n",
    "\n",
    "```bash\n",
    "python scripts/sr_classification.py --dataset ptb-xl --positive-class HYP\n",
    "python scripts/sr_classification.py --dataset ptb-xl --all  # Tutte le patologie\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c29ef29",
   "metadata": {},
   "source": [
    "# üìä Analisi Risultati SR Classification\n",
    "\n",
    "## Sommario Esecutivo\n",
    "\n",
    "Ho analizzato 18 classificatori SR (5 patologie √ó 4 dataset, meno alcuni non disponibili). Ecco i risultati organizzati per patologia.\n",
    "\n",
    "---\n",
    "\n",
    "## ü´Ä **CD (Conduction Disturbances)** - Disturbi di Conduzione\n",
    "\n",
    "| Dataset | AUC SR | AUC Baseline | Formula | Complessit√† |\n",
    "|---------|--------|--------------|---------|-------------|\n",
    "| **Chapman** | **0.946** | 0.970 | `0.75 * log(rr_std_ms + heart_rate_bpm + 2.70)` | 8 |\n",
    "| CPSC-2018 | 0.902 | 0.922 | `log(age + rr_std_ms + 3.60) * 0.61` | 8 |\n",
    "| PTB-XL | 0.789 | 0.837 | `((abs(rr_std_ms) + heart_rate_bpm) - snr_db) * 0.15` | 10 |\n",
    "| Georgia | 0.822 | 0.901 | `(n_beats + age) * 0.19 + 0.5` | 7 |\n",
    "\n",
    "**üîë Pattern comune**: `log(rr_std_ms + heart_rate_bpm)` o combinazioni di **variabilit√† RR + frequenza cardiaca**\n",
    "\n",
    "**Interpretazione**: I disturbi di conduzione alterano la regolarit√† del ritmo cardiaco. La formula cattura questo attraverso:\n",
    "- `rr_std_ms`: variabilit√† degli intervalli RR (alta nei blocchi)\n",
    "- `heart_rate_bpm`: frequenza (spesso alterata)\n",
    "- `age`: fattore di rischio noto\n",
    "\n",
    "---\n",
    "\n",
    "## üî¥ **MI (Myocardial Infarction)** - Infarto\n",
    "\n",
    "| Dataset | AUC SR | AUC Baseline | Formula | Complessit√† |\n",
    "|---------|--------|--------------|---------|-------------|\n",
    "| **PTB-XL** | **0.838** | 0.886 | `((age - wav_qrs_peak) + 4.28 - (quality - HR)) * 0.12` | 11 |\n",
    "| Chapman | 0.820 | 0.896 | `(heart_rate_bpm + age) * 0.18 + 0.5` | 7 |\n",
    "| Georgia | 0.771 | 0.793 | `(abs(rr_std_ms) - gzip_ratio + n_beats) * 0.14` | 10 |\n",
    "\n",
    "**üîë Pattern comune**: `age + heart_rate_bpm` con possibile modulazione wavelet\n",
    "\n",
    "**Interpretazione**: L'infarto √® associato a:\n",
    "- `age`: forte fattore di rischio\n",
    "- `wav_wavelet_qrs_peak`: alterazioni morfologiche del QRS\n",
    "- `heart_rate_bpm`: tachicardia compensatoria\n",
    "\n",
    "---\n",
    "\n",
    "## üìà **STTC (ST/T Changes)** - Alterazioni ST-T\n",
    "\n",
    "| Dataset | AUC SR | AUC Baseline | Formula | Complessit√† |\n",
    "|---------|--------|--------------|---------|-------------|\n",
    "| **PTB-XL** | **0.825** | 0.875 | `log(sqrt(wav_kurtosis + age + HR + 2.99))` | 9 |\n",
    "| Chapman | 0.787 | 0.846 | `(heart_rate_bpm + 2.91 + age) * 0.17` | 7 |\n",
    "| CPSC-2018 | 0.817 | 0.890 | `0.20 * (hjorth_complexity + 2.55 - sex)` | 7 |\n",
    "| Georgia | 0.735 | 0.831 | `(n_beats + age) * 0.11 + 0.68` | 7 |\n",
    "\n",
    "**üîë Pattern comune**: `age + heart_rate_bpm` con features wavelet/complessit√†\n",
    "\n",
    "**Interpretazione**: Le alterazioni ST-T sono catturate da:\n",
    "- `wav_wavelet_coef_kurtosis`: forma anomala dell'onda (code pesanti)\n",
    "- `age`: degenerazione tissutale\n",
    "- `hjorth_complexity`: complessit√† del segnale\n",
    "\n",
    "---\n",
    "\n",
    "## üí™ **HYP (Hypertrophy)** - Ipertrofia\n",
    "\n",
    "| Dataset | AUC SR | AUC Baseline | Formula | Complessit√† |\n",
    "|---------|--------|--------------|---------|-------------|\n",
    "| **Chapman** | **0.959** | 0.918 | `(sqrt(abs(wav_energy_approx)) + kurtosis - (coef_min - abs(-1.17 - age))) * 0.22` | 14 |\n",
    "| PTB-XL | 0.669 | 0.701 | `wav_wavelet_coef_max * 0.13 + 0.5` | 5 |\n",
    "| Georgia | 0.573 | 0.642 | `wav_wavelet_coef_min * -0.11 + 0.5` | 5 |\n",
    "\n",
    "**‚ö†Ô∏è Nota**: Dataset molto sbilanciati (pochi HYP), risultati meno affidabili\n",
    "\n",
    "**üîë Pattern comune**: `wav_wavelet_coef_max/min` - ampiezza delle onde\n",
    "\n",
    "**Interpretazione**: L'ipertrofia causa voltaggio aumentato:\n",
    "- `wav_wavelet_coef_max`: ampiezza massima (tipicamente alta in LVH)\n",
    "- `wav_wavelet_coef_min`: profondit√† onde (inversione in RVH)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùì **OTHER** - Altre patologie\n",
    "\n",
    "| Dataset | AUC SR | AUC Baseline | Formula | Complessit√† |\n",
    "|---------|--------|--------------|---------|-------------|\n",
    "| CPSC-2018 | 0.783 | 0.844 | `0.77 + (hjorth_complexity + age) / 8.87` | 7 |\n",
    "| Georgia | 0.727 | 0.771 | `log((heart_rate_std - snr_db) * 0.26 + 1.68)` | 8 |\n",
    "| Chapman | 0.665 | 0.665 | `(square(age) + 5.05 - wav_coef_min) * 0.08` | 8 |\n",
    "| PTB-XL | 0.584 | 0.509 | `0.70 / (wav_total_energy + 1.56)` | 5 |\n",
    "\n",
    "**‚ö†Ô∏è Nota**: Categoria eterogenea, difficile da classificare\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Classifica Performance per Patologia\n",
    "\n",
    "| Patologia | Miglior AUC | Dataset | Formula Candidata |\n",
    "|-----------|-------------|---------|-------------------|\n",
    "| **CD** | 0.946 | Chapman | `log(rr_std_ms + heart_rate_bpm + c)` |\n",
    "| **MI** | 0.838 | PTB-XL | `(age - wav_qrs_peak + c) * k` |\n",
    "| **STTC** | 0.825 | PTB-XL | `log(sqrt(wav_kurtosis + age + HR))` |\n",
    "| **HYP** | 0.959* | Chapman | `wav_wavelet_coef_max` (semplificato) |\n",
    "| **OTHER** | 0.783 | CPSC-2018 | `hjorth_complexity + age` |\n",
    "\n",
    "*\\*AUC alta ma su dataset molto piccolo (14 test samples)*\n",
    "\n",
    "---\n",
    "\n",
    "## üß¨ Features Pi√π Importanti (Cross-Dataset)\n",
    "\n",
    "| Feature | Patologie | Significato Clinico |\n",
    "|---------|-----------|---------------------|\n",
    "| `age` | MI, STTC, CD, HYP | Fattore di rischio universale |\n",
    "| `heart_rate_bpm` | CD, MI, STTC | Risposta fisiologica/patologica |\n",
    "| `rr_std_ms` | CD, MI | Variabilit√† ritmica (aritmie) |\n",
    "| `wav_wavelet_coef_max/min` | HYP | Ampiezza voltaggio (ipertrofia) |\n",
    "| `wav_wavelet_coef_kurtosis` | STTC | Forma anomala onde |\n",
    "| `hjorth_complexity` | STTC, OTHER | Complessit√† segnale |\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Raccomandazioni per Formula Universale\n",
    "\n",
    "Per ogni patologia, propongo una **formula candidata semplificata** da validare cross-dataset:\n",
    "\n",
    "```python\n",
    "# CD (Conduction Disturbances)\n",
    "CD_score = log(rr_std_ms + heart_rate_bpm + 3)\n",
    "\n",
    "# MI (Myocardial Infarction)  \n",
    "MI_score = (age + heart_rate_bpm) * 0.15\n",
    "\n",
    "# STTC (ST/T Changes)\n",
    "STTC_score = log(sqrt(wav_wavelet_coef_kurtosis + age + heart_rate_bpm + 3))\n",
    "\n",
    "# HYP (Hypertrophy)\n",
    "HYP_score = wav_wavelet_coef_max * 0.12 + 0.5\n",
    "\n",
    "# OTHER\n",
    "OTHER_score = (hjorth_complexity + age) / 10\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Limitazioni Osservate\n",
    "\n",
    "1. **SR vs Baseline**: Le formule SR sono generalmente **5-10% sotto** il baseline (Logistic Regression), ma sono **interpretabili**\n",
    "2. **Dataset sbilanciati**: HYP ha pochissimi campioni in alcuni dataset\n",
    "3. **Variabilit√† cross-dataset**: Le formule migliori variano per dataset (popolazioni diverse)\n",
    "4. **Features mancanti**: Le features `interval_*` (PR, QT, QRS) sono vuote - potrebbero migliorare i risultati"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149e6f67",
   "metadata": {},
   "source": [
    "## üî¨ Analisi Cross-Dataset Completata\n",
    "\n",
    "### **CD (Conduction Disturbances)** - Formula Universale Candidata\n",
    "\n",
    "| Complessit√† | Formula | Georgia AUC | Pattern Cross-Dataset |\n",
    "|-------------|---------|-------------|----------------------|\n",
    "| **7** | `(n_beats + age) * 0.19 + 0.5` | **0.822** | ‚úÖ Presente in tutti |\n",
    "| 8 | `log(rr_std_ms + heart_rate_bpm + c)` | 0.802 | Chapman: 0.946 |\n",
    "\n",
    "**üéØ Formula Universale CD:**\n",
    "```\n",
    "CD_score = (n_beats + age) * k + 0.5\n",
    "```\n",
    "Dove `k ‚âà 0.19`. Semplice, interpretabile, cross-dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **MI (Myocardial Infarction)** - Formula Universale Candidata\n",
    "\n",
    "| Complessit√† | Formula | Georgia AUC | Pattern Cross-Dataset |\n",
    "|-------------|---------|-------------|----------------------|\n",
    "| **7** | `(n_beats + wav_wavelet_coef_kurtosis) * 0.18 + 0.5` | **0.690** | ‚úÖ |\n",
    "| 10 | `(abs(rr_std_ms) - gzip_ratio + n_beats) * 0.14` | 0.771 | ‚úÖ |\n",
    "\n",
    "**üéØ Formula Universale MI:**\n",
    "```\n",
    "MI_score = (n_beats + wav_wavelet_coef_kurtosis) * k + 0.5\n",
    "```\n",
    "oppure versione pi√π complessa:\n",
    "```\n",
    "MI_score = (rr_std_ms + n_beats - comp_sig_gzip_ratio) * k\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **STTC (ST/T Changes)** - Formula Universale Candidata\n",
    "\n",
    "| Complessit√† | Formula | Georgia AUC | Pattern Cross-Dataset |\n",
    "|-------------|---------|-------------|----------------------|\n",
    "| **7** | `(n_beats + age) * 0.11 + 0.68` | **0.735** | ‚úÖ Identica a CD! |\n",
    "| 8 | `log(sqrt(abs(rr_std_ms) + n_beats + c))` | 0.729 | ‚úÖ |\n",
    "\n",
    "**üéØ Formula Universale STTC:**\n",
    "```\n",
    "STTC_score = (n_beats + age) * k + c\n",
    "```\n",
    "Molto simile a CD, differisce solo nei coefficienti.\n",
    "\n",
    "---\n",
    "\n",
    "### **HYP (Hypertrophy)** - Formula Universale Candidata\n",
    "\n",
    "| Complessit√† | Formula | Georgia AUC | Pattern Cross-Dataset |\n",
    "|-------------|---------|-------------|----------------------|\n",
    "| **5** | `wav_wavelet_coef_min * -0.11 + 0.5` | **0.573** | ‚úÖ PTB-XL simile |\n",
    "| 7 | `(wav_coef_min - hjorth_complexity) * -0.12 + 0.5` | 0.561 | ‚úÖ |\n",
    "\n",
    "**üéØ Formula Universale HYP:**\n",
    "```\n",
    "HYP_score = wav_wavelet_coef_min * (-k) + 0.5\n",
    "```\n",
    "oppure:\n",
    "```\n",
    "HYP_score = (wav_wavelet_coef_min - comp_sig_hjorth_complexity) * (-k) + 0.5\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **OTHER** - Formula Universale Candidata\n",
    "\n",
    "| Complessit√† | Formula | Georgia AUC | Pattern Cross-Dataset |\n",
    "|-------------|---------|-------------|----------------------|\n",
    "| **8** | `log((heart_rate_std - snr_db) * 0.26 + 1.68)` | **0.727** | ‚úÖ |\n",
    "| 5 | `1.41 / (quality_score + 3.14)` | 0.647 | Diverso |\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Riepilogo Formule Universali Proposte\n",
    "\n",
    "| Patologia | Formula Semplificata | Complessit√† | AUC Range |\n",
    "|-----------|---------------------|-------------|-----------|\n",
    "| **CD** | `(n_beats + age) * 0.19` | 7 | 0.75-0.95 |\n",
    "| **MI** | `(n_beats + wav_kurtosis) * 0.18` | 7 | 0.69-0.84 |\n",
    "| **STTC** | `(n_beats + age) * 0.11` | 7 | 0.68-0.83 |\n",
    "| **HYP** | `wav_coef_min * (-0.11)` | 5 | 0.57-0.96* |\n",
    "| **OTHER** | `log(heart_rate_std - snr_db)` | 8 | 0.58-0.78 |\n",
    "\n",
    "*HYP ha alta variabilit√† per dataset sbilanciati*\n",
    "\n",
    "---\n",
    "\n",
    "## üß¨ Pattern Cross-Dataset Pi√π Robusti\n",
    "\n",
    "### Features Universali (presenti in tutte le patologie):\n",
    "1. **`n_beats`** - Numero di battiti (proxy per durata/qualit√†)\n",
    "2. **`age`** - Fattore di rischio universale\n",
    "\n",
    "### Features Patologia-Specifiche:\n",
    "- **CD**: `rr_std_ms` (variabilit√† ritmo)\n",
    "- **MI**: `wav_wavelet_coef_kurtosis`, `comp_sig_gzip_ratio`\n",
    "- **STTC**: `quality_score`, `wav_wavelet_coef_kurtosis`\n",
    "- **HYP**: `wav_wavelet_coef_min`, `comp_sig_hjorth_complexity`\n",
    "- **OTHER**: `heart_rate_std`, `snr_db`\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Osservazione Chiave\n",
    "\n",
    "**CD e STTC hanno formule quasi identiche!**\n",
    "```\n",
    "CD:   (n_beats + age) * 0.19 + 0.5\n",
    "STTC: (n_beats + age) * 0.11 + 0.68\n",
    "```\n",
    "\n",
    "Questo suggerisce che:\n",
    "1. La struttura della formula √® robusta cross-dataset\n",
    "2. Solo i coefficienti cambiano per patologia\n",
    "3. Una **formula parametrica** potrebbe funzionare per pi√π patologie\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706ccd9c",
   "metadata": {},
   "source": [
    "### Step 4.3: Stream B - Symbolic Regression for Cardiac Age Prediction\n",
    "\n",
    "**Script**: [scripts/sr_cardiac_age.py](../scripts/sr_cardiac_age.py)\n",
    "\n",
    "This module is a core discovery component of the **Kepler-ECG** project, specifically designed for **Cardiac Age Regression**. Its purpose is to evolve transparent, interpretable mathematical formulas that predict a subject's biological age based on ECG features. The primary clinical value lies in the **\"Cardiac Age Delta\"** (predicted age vs. chronological age): a heart that appears \"older\" than the patient's actual age can serve as an early biomarker for cardiovascular decline. Utilizing the `PySR` engine, the script searches for the most parsimonious equations that balance predictive accuracy with medical explainability.\n",
    "\n",
    "#### Input\n",
    "\n",
    "The script accepts raw feature data and configuration parameters via the `run_sr_cardiac_age` function or CLI:\n",
    "\n",
    "* **`df` (pd.DataFrame)**: A dataset containing extracted ECG features (morphological, spectral, wavelet) and a target age column.\n",
    "* **`age_col` (str)**: The name of the target variable (default: `'age'`).\n",
    "* **`norm_only` (bool)**: If true, the model is trained exclusively on healthy subjects (`NORM`) to establish a baseline for \"ideal\" cardiac aging.\n",
    "* **`SRConfig`**: Evolutionary parameters including:\n",
    "* **`niterations`**: Number of search cycles.\n",
    "* **`maxsize`**: Maximum complexity of the formula (number of nodes).\n",
    "* **Operators**: Selection of mathematical building blocks (e.g., `plus`, `mult`, `log`, `exp`, `sqrt`).\n",
    "\n",
    "#### Output\n",
    "\n",
    "The module generates a comprehensive discovery package in the specified results directory:\n",
    "\n",
    "* **`sr_age_results.json`**: A master file containing the \"Best Equation,\" its  score, Mean Absolute Error (MAE), and the parameters used for the search.\n",
    "* **`age_hall_of_fame.csv`**: The full Pareto front of candidate equations, allowing researchers to choose between a simpler formula or a more complex, accurate one.\n",
    "* **Validation Metrics**:\n",
    "* **Comparison vs. Baseline**: Performance relative to standard Linear or Ridge regression.\n",
    "* **Error Analysis**: Distribution of the \"Cardiac Age Delta\" across the population.\n",
    "\n",
    "\n",
    "* **Visualizations**:\n",
    "* **Scatter Plots**: Predicted vs. Actual Age.\n",
    "* **Residual Plots**: Highlighting where the model overestimates or underestimates cardiac age.\n",
    "\n",
    "#### Running\n",
    "\n",
    "```bash\n",
    "python scripts/sr_cardiac_age.py --dataset ptb-xl\n",
    "python scripts/sr_cardiac_age.py --dataset ptb-xl --norm-only\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6344fd5d",
   "metadata": {},
   "source": [
    "## üìä Analisi SR Cardiac Age - Risultati\n",
    "\n",
    "### Sommario per Dataset\n",
    "\n",
    "| Dataset | N Samples | R¬≤ SR | R¬≤ Baseline | MAE | Formula Migliore | Complessit√† |\n",
    "|---------|-----------|-------|-------------|-----|------------------|-------------|\n",
    "| **PTB-XL** | 21,364 | 0.196 | 0.192 | 12.18 | `hjorth + lempel_ziv - wav_max` | 18 |\n",
    "| **Chapman** | 42,863 | 0.228 | 0.264 | 12.84 | `lempel_ziv - wav_d4 + rr_std - rr_mean` | 13 |\n",
    "| **CPSC-2018** | 6,724 | 0.100 | 0.175 | 13.82 | `exp(2.06 - wav_coef_max) + 50.6` | 6 |\n",
    "| **Georgia** | 10,256 | 0.145 | 0.126 | 11.36 | `lzma_ratio + n_beats + rr_std - wav_min` | 17 |\n",
    "\n",
    "---\n",
    "\n",
    "### üî¨ Analisi Dettagliata\n",
    "\n",
    "#### **PTB-XL** (Best R¬≤ = 0.196)\n",
    "```\n",
    "cardiac_age = abs((22.32 / (heart_rate_std + 0.96)) - 24.14) \n",
    "            + ((hjorth_complexity + lempel_ziv + 18.86) - wav_coef_max) * 2.71\n",
    "```\n",
    "- SR leggermente migliore del baseline (+0.4% R¬≤)\n",
    "- Features chiave: `heart_rate_std`, `comp_sig_hjorth_complexity`, `comp_sig_lempel_ziv_complexity`\n",
    "\n",
    "#### **Chapman** (Best R¬≤ = 0.228)\n",
    "```\n",
    "cardiac_age = ((lempel_ziv - wav_energy_d4 + rr_std - rr_mean) - wav_coef_std) * 3.79 + 61.22\n",
    "```\n",
    "- SR sotto il baseline (-3.6% R¬≤)\n",
    "- Formula pi√π semplice (complessit√† 13)\n",
    "- Features chiave: `comp_sig_lempel_ziv_complexity`, `rr_std_ms`, `rr_mean_ms`\n",
    "\n",
    "#### **CPSC-2018** (Best R¬≤ = 0.100)\n",
    "```\n",
    "cardiac_age = exp(2.06 - wav_wavelet_coef_max) + 50.64\n",
    "```\n",
    "- **Formula pi√π semplice!** (complessit√† 6)\n",
    "- SR sotto il baseline (-7.5% R¬≤)\n",
    "- Usa solo `wav_wavelet_coef_max`\n",
    "\n",
    "#### **Georgia** (Best R¬≤ = 0.145)\n",
    "```\n",
    "cardiac_age = ((lzma_ratio + abs(n_beats + rr_std) - wav_coef_min) * 3.42) \n",
    "            + (square(wav_coef_mean_abs - 4.15) + 38.37)\n",
    "```\n",
    "- **SR migliore del baseline** (+2% R¬≤)\n",
    "- Features chiave: `comp_sig_lzma_ratio`, `n_beats`, `rr_std_ms`\n",
    "\n",
    "---\n",
    "\n",
    "### üß¨ Pattern Cross-Dataset\n",
    "\n",
    "#### Features Pi√π Frequenti nelle Formule:\n",
    "\n",
    "| Feature | PTB-XL | Chapman | CPSC-2018 | Georgia | Totale |\n",
    "|---------|--------|---------|-----------|---------|--------|\n",
    "| `comp_sig_lempel_ziv` | ‚úÖ | ‚úÖ | | | 2 |\n",
    "| `comp_sig_lzma_ratio` | | | | ‚úÖ | 1 |\n",
    "| `wav_wavelet_coef_max` | ‚úÖ | | ‚úÖ | | 2 |\n",
    "| `wav_wavelet_coef_min` | | | | ‚úÖ | 1 |\n",
    "| `wav_wavelet_coef_std` | | ‚úÖ | | | 1 |\n",
    "| `rr_std_ms` | | ‚úÖ | | ‚úÖ | 2 |\n",
    "| `heart_rate_std` | ‚úÖ | | | | 1 |\n",
    "| `hjorth_complexity` | ‚úÖ | | | | 1 |\n",
    "| `n_beats` | | | | ‚úÖ | 1 |\n",
    "\n",
    "#### Pattern Comune Identificato:\n",
    "```\n",
    "cardiac_age ‚âà (compressibility_feature - wavelet_amplitude) * k + offset\n",
    "```\n",
    "\n",
    "Dove:\n",
    "- **compressibility_feature**: `lempel_ziv`, `lzma_ratio`, `hjorth_complexity`\n",
    "- **wavelet_amplitude**: `wav_coef_max`, `wav_coef_min`, `wav_coef_std`\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Cardiac Age Delta Analysis\n",
    "\n",
    "| Dataset | Mean Œî | Std Œî | % Older | % Younger | Age Trend Slope |\n",
    "|---------|--------|-------|---------|-----------|-----------------|\n",
    "| PTB-XL | +0.46 | 15.14 | 49.9% | 50.1% | **-0.80** |\n",
    "| Chapman | +0.03 | 15.89 | 45.9% | 54.1% | **-0.78** |\n",
    "| CPSC-2018 | -0.01 | 16.94 | 45.1% | 54.9% | **-0.90** |\n",
    "| Georgia | +0.40 | 14.17 | 47.9% | 52.1% | **-0.86** |\n",
    "\n",
    "**Age Trend Slope ‚âà -0.84** (consistente cross-dataset)\n",
    "\n",
    "Questo significa: per ogni anno di et√† reale in pi√π, l'et√† cardiaca predetta aumenta solo di ~0.84 anni. C'√® una **regressione verso la media** - i giovani vengono sovrastimati, gli anziani sottostimati.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Formula Universale Candidata per Cardiac Age\n",
    "\n",
    "Basandomi sui pattern comuni, propongo:\n",
    "\n",
    "```python\n",
    "# Formula semplice (complessit√† ~8)\n",
    "cardiac_age = (comp_sig_lempel_ziv - wav_wavelet_coef_max) * k + mean_age\n",
    "\n",
    "# Formula estesa (complessit√† ~12)  \n",
    "cardiac_age = ((lempel_ziv + rr_std_ms) - wav_coef_max) * k + offset\n",
    "```\n",
    "\n",
    "Con `k ‚âà 3-4` e `offset ‚âà 50-60` (et√† media della popolazione).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Limitazioni Osservate\n",
    "\n",
    "1. **R¬≤ basso** (0.10-0.26): L'et√† √® difficile da predire solo da features ECG\n",
    "2. **SR ‚â§ Baseline** in 2/4 dataset: Le formule semplici non sempre battono Ridge/Linear\n",
    "3. **Alta variabilit√†** (MAE ~12-14 anni): Errore medio di ¬±12 anni\n",
    "4. **Regressione verso la media**: Slope ~0.84 invece di 1.0\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Interpretazione Clinica\n",
    "\n",
    "Le features che predicono l'et√† cardiaca sono legate a:\n",
    "\n",
    "1. **Complessit√† del segnale** (`lempel_ziv`, `lzma_ratio`, `hjorth`):\n",
    "   - Segnali pi√π \"complessi\" ‚Üí et√† maggiore\n",
    "   - Perdita di regolarit√† con l'invecchiamento\n",
    "\n",
    "2. **Ampiezza wavelet** (`wav_coef_max`, `wav_coef_min`):\n",
    "   - Ampiezze minori ‚Üí et√† maggiore\n",
    "   - Riduzione voltaggio QRS con l'et√†\n",
    "\n",
    "3. **Variabilit√† RR** (`rr_std_ms`):\n",
    "   - Minore variabilit√† ‚Üí et√† maggiore\n",
    "   - Perdita di modulazione autonomica\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997af8ad",
   "metadata": {},
   "source": [
    "üî¨ Analisi Cross-Dataset Completa - Cardiac Age\n",
    "\n",
    "### üìä Features Pi√π Frequenti per Dataset\n",
    "\n",
    "| Feature | PTB-XL | Chapman | CPSC-2018 | Georgia | **Totale** |\n",
    "|---------|--------|---------|-----------|---------|------------|\n",
    "| `comp_sig_lempel_ziv` | 7 | **17** | - | - | 24 |\n",
    "| `comp_sig_kolmogorov` | 1 | - | **17** | 3 | 21 |\n",
    "| `rr_std_ms` | - | **15** | 10 | **15** | 40 |\n",
    "| `comp_sig_hjorth_complexity` | **11** | 4 | - | - | 15 |\n",
    "| `comp_sig_bzip2_ratio` | 6 | - | 9 | - | 15 |\n",
    "| `comp_sig_lzma_ratio` | - | - | - | **11** | 11 |\n",
    "| `wav_wavelet_coef_max` | 9 | 3 | 1 | - | 13 |\n",
    "| `wav_wavelet_coef_min` | - | 6 | - | **11** | 17 |\n",
    "| `wav_wavelet_energy_d4` | 2 | **18** | 9 | 1 | 30 |\n",
    "| `heart_rate_std` | **9** | - | - | - | 9 |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Formule Semplici (C=5-8) - Confronto Cross-Dataset\n",
    "\n",
    "| Dataset | C | R¬≤ | MAE | Formula |\n",
    "|---------|---|-----|-----|---------|\n",
    "| **PTB-XL** | 8 | **0.107** | 12.85 | `(wav_energy_d3 - 7.47) / exp(hjorth) + 70` |\n",
    "| **Chapman** | 7 | **0.130** | 13.82 | `(lempel_ziv + 12.96 - wav_energy_d4) * 4.73` |\n",
    "| **CPSC-2018** | 6 | **0.100** | 13.82 | `exp(2.06 - wav_coef_max) + 50.6` |\n",
    "| **Georgia** | 8 | **0.095** | 11.81 | `square(2.90 - qrs_variability) + lzma_ratio + 50.5` |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Formule Medie (C=9-13) - Confronto Cross-Dataset\n",
    "\n",
    "| Dataset | C | R¬≤ | MAE | Formula Pattern |\n",
    "|---------|---|-----|-----|-----------------|\n",
    "| **PTB-XL** | 12 | **0.137** | 12.58 | `abs(k / (heart_rate_std + c) - c2) + bzip2 + 50` |\n",
    "| **Chapman** | 13 | **0.228** | 12.84 | `((lempel_ziv - wav_d4 + rr_std - rr_mean) - wav_std) * 3.8 + 61` |\n",
    "| **CPSC-2018** | 13 | **0.106** | 13.65 | `rr_std + (kolmogorov - qrs_peak*2) * (bzip2 + 5.5) + 59` |\n",
    "| **Georgia** | 11 | **0.112** | 11.67 | `((rr_std + lzma - qrs_var - coef_mean_abs) * 2.1) + 60` |\n",
    "\n",
    "---\n",
    "\n",
    "### üß¨ Pattern Universale Identificato\n",
    "\n",
    "Analizzando tutte le formule, emerge un **pattern strutturale comune**:\n",
    "\n",
    "```\n",
    "cardiac_age = (compressibility - wavelet_amplitude + rr_variability) * k + mean_age\n",
    "```\n",
    "\n",
    "Dove:\n",
    "- **compressibility**: `lempel_ziv`, `kolmogorov`, `lzma_ratio`, `bzip2_ratio`, `hjorth_complexity`\n",
    "- **wavelet_amplitude**: `wav_coef_max`, `wav_coef_min`, `wav_coef_std`, `wav_energy_d4`\n",
    "- **rr_variability**: `rr_std_ms`, `rr_mean_ms` (differenza)\n",
    "- **k** ‚âà 2-5\n",
    "- **mean_age** ‚âà 50-61\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Riepilogo Performance\n",
    "\n",
    "| Dataset | Best R¬≤ | Best MAE | Complessit√† Ottimale |\n",
    "|---------|---------|----------|---------------------|\n",
    "| PTB-XL | 0.204 | 12.09 | 20 |\n",
    "| Chapman | 0.257 | 12.62 | 25 |\n",
    "| CPSC-2018 | 0.157 | 13.26 | 23 |\n",
    "| Georgia | 0.154 | 11.31 | 20 |\n",
    "\n",
    "**Media cross-dataset**: R¬≤ ‚âà 0.19, MAE ‚âà 12.3 anni\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Formula Universale Candidata\n",
    "\n",
    "#### **Formula Semplice (C‚âà7-8)**\n",
    "```python\n",
    "cardiac_age = (comp_sig_lempel_ziv - wav_wavelet_coef_max + c1) * k + mean_age\n",
    "# Esempio: (lempel_ziv - wav_max + 13) * 4.7\n",
    "```\n",
    "- R¬≤ atteso: 0.08-0.13\n",
    "- MAE atteso: 12-14 anni\n",
    "\n",
    "#### **Formula Bilanciata (C‚âà11-13)**\n",
    "```python\n",
    "cardiac_age = ((rr_std_ms + compressibility) - wav_amplitude) * k + mean_age\n",
    "# Esempio: ((rr_std + lempel_ziv - wav_energy_d4 - wav_coef_max) * 3.5) + 60\n",
    "```\n",
    "- R¬≤ atteso: 0.11-0.23\n",
    "- MAE atteso: 11.5-13.5 anni\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Interpretazione Clinica Confermata\n",
    "\n",
    "Il pattern cross-dataset conferma che l'**et√† cardiaca** √® predetta da:\n",
    "\n",
    "1. **‚Üë Compressibilit√†** (segnale pi√π \"regolare\", meno informazione) ‚Üí **pi√π vecchio**\n",
    "   - `lempel_ziv`, `kolmogorov`, `lzma_ratio` aumentano con et√†\n",
    "\n",
    "2. **‚Üì Ampiezza wavelet** (voltaggio ridotto) ‚Üí **pi√π vecchio**\n",
    "   - `wav_coef_max`, `wav_energy_d4` diminuiscono con et√†\n",
    "\n",
    "3. **‚Üì Variabilit√† RR** (perdita modulazione autonomica) ‚Üí **pi√π vecchio**\n",
    "   - `rr_std_ms` diminuisce con et√†\n",
    "\n",
    "**Formula interpretabile finale:**\n",
    "```\n",
    "et√†_cardiaca ‚âà 60 + 3.5 √ó (complessit√†_segnale - ampiezza_onde + variabilit√†_RR_ridotta)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a17a634",
   "metadata": {},
   "source": [
    "### Step 4.4: Multi-Dataset Wave Delineation Pipeline\n",
    "\n",
    "**Script**: [scripts/extract_waves.py](../scripts/extract_waves.py)\n",
    "\n",
    "The **Multi-Dataset Wave Delineation Pipeline** is a robust signal processing engine designed to extract precise PQRST fiducial points from various ECG databases (PTB-XL, Chapman, CPSC, etc.). By leveraging the `NeuroKit2` library and `wfdb` format support, it identifies the onset, peak, and offset of cardiac waves. This module is foundational for **Phase 4.5**, as it converts raw voltage-time signals into clinical interval metrics (RR, PR, QRS, QT) and automatically calculates standard heart-rate corrected intervals (QTc) using multiple clinical formulas.\n",
    "\n",
    "#### Input\n",
    "\n",
    "The script is designed for high-volume batch processing and accepts:\n",
    "\n",
    "* **`--dataset` (str)**: The registry name of the dataset to process (e.g., `'ptb-xl'`, `'chapman'`).\n",
    "* **`--input` (Path)**: An optional direct path to a directory containing ECG records (overrides the registry).\n",
    "* **`--sampling_rate` (int, default: 500)**: The frequency of the input signal to ensure accurate time-series analysis.\n",
    "* **`--lead` (int, default: 0)**: The lead index (e.g., Lead I or Lead II) to be used as the primary source for delineation.\n",
    "* **`--max_records` (Optional)**: A limit for the number of files to process, ideal for debugging or sampling.\n",
    "\n",
    "#### Output\n",
    "\n",
    "The pipeline produces a comprehensive results package in the specified output directory:\n",
    "\n",
    "* **`wave_features.csv`**: A dense feature matrix containing:\n",
    "* **Fiducial Points**: Sample indices for P-peak, QRS-onset, R-peak, T-offset, etc.\n",
    "* **Calculated Intervals**: RR, PR, QRS, and QT durations in milliseconds.\n",
    "* **Baseline QTc Values**: Pre-calculated Bazett, Fridericia, Framingham, and Hodges corrections.\n",
    "\n",
    "\n",
    "* **`delineation_summary.json`**: A master metadata file reporting:\n",
    "* **Processing Stats**: Total records processed vs. successful delineations.\n",
    "* **Population Statistics**: Mean and standard deviation for all extracted intervals and heart rates.\n",
    "* **Success Rate**: A percentage-based reliability score for the dataset quality.\n",
    "\n",
    "\n",
    "* **Console Telemetry**: A real-time progress bar (tqdm) and final summary table highlighting clinical averages.\n",
    "\n",
    "#### Running\n",
    "\n",
    "```bash\n",
    "python scripts/extract_waves.py --dataset ptb-xl\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777750ae",
   "metadata": {},
   "source": [
    "### Step 4.5: QTc Dataset Preparation Utility\n",
    "\n",
    "**Script**: [scripts/prepare_qtc_dataset.py](../scripts/prepare_qtc_dataset.py)\n",
    "\n",
    "The **QTc Dataset Preparation Module** is a robust data engineering script designed to transform raw wave features into high-quality datasets for medical formula discovery. It implements a multi-stage filtering pipeline that enforces clinical quality standards (e.g., valid QT and RR ranges) and integrates diagnostic metadata to categorize patients. A key feature of this module is the calculation of a \"Ground Truth\" QTc reference‚Äîan idealized QT interval corrected to a heart rate of 60 bpm‚Äîusing population-level regression. This ensures the resulting data is perfectly formatted for training **Symbolic Regression** models to discover next-generation heart rate correction formulas.\n",
    "\n",
    "#### Input\n",
    "\n",
    "The script can be executed via CLI or integrated as a module, requiring:\n",
    "\n",
    "* **`--input` (Path)**: The CSV file generated by the wave extraction pipeline (e.g., `wave_features.csv`), containing raw millisecond measurements of PQRST landmarks.\n",
    "* **`--dataset` (str)**: The name of the registry dataset (e.g., `'ptb-xl'`, `'chapman'`) to automatically locate metadata and labels.\n",
    "* **Quality Thresholds**: Internally defined ranges for clinical physiological limits (e.g., heart rate between 30 and 200 BPM, QT intervals between 200ms and 800ms).\n",
    "\n",
    "#### Output\n",
    "\n",
    "The module produces a refined \"SR-Ready\" (Symbolic Regression Ready) package:\n",
    "\n",
    "* **`qtc_sr_dataset_all.csv`**: The complete cleaned dataset including original intervals, standard corrections (Bazett, Fridericia), and the calculated reference.\n",
    "* **`qtc_sr_dataset_norm.csv`**: A specialized subset containing only \"Healthy\" (NORM) subjects, ideal for learning baseline cardiac physiology.\n",
    "* **`qtc_prep_report.json`**: A detailed analytical summary featuring:\n",
    "* **Filter Statistics**: A breakdown of how many records were dropped due to noise or physiological outliers.\n",
    "* **Correlation Analysis**: Pearson 'r' values showing how much traditional formulas still depend on heart rate.\n",
    "* **Dataset Sizes**: Final counts for training and validation cohorts.\n",
    "\n",
    "\n",
    "* **Console Logs**: Real-time feedback on the normalization process and the strength of the HR-QT correlation in the processed batch.\n",
    "\n",
    "#### Running\n",
    "\n",
    "```bash\n",
    "python scripts/prepare_qtc_dataset.py --dataset ptb-xl\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ac2c36",
   "metadata": {},
   "source": [
    "### Step 4.6: Stream C - QTc Formula Discovery via Symbolic Regression\n",
    "\n",
    "**Script**: [scripts/sr_qtc_discovery.py](../scripts/sr_qtc_discovery.py)\n",
    "\n",
    "The **QTc Discovery Module** is an advanced scientific tool within Phase 4.5 of the Kepler-ECG project. It leverages **Symbolic Regression (via PySR)** to discover interpretable mathematical formulas for heart rate-corrected QT intervals (QTc). The script moves beyond standard clinical rules (like Bazett or Fridericia) by exploring three mathematical architectures: **Direct prediction** (formula maps raw features to QTc), **Factor-based** (QT multiplied by a correction function), and **Additive** (QT plus a correction term). The primary objective is to evolve a formula that minimizes the correlation between the corrected QT and heart rate, providing a more stable diagnostic metric for different physiological states.\n",
    "\n",
    "#### Input\n",
    "\n",
    "The script is designed for automated execution using pre-prepared datasets:\n",
    "\n",
    "* **`--dataset` (str)**: The name of the registered dataset (e.g., `'ptb-xl'`) to automatically fetch the prepared CSV.\n",
    "* **`--input` (Path)**: Alternatively, a direct path to an SR-ready CSV (e.g., `qtc_sr_dataset_all.csv`) containing cleaned QT and RR intervals.\n",
    "* **`--approach` (str, default: 'all')**: Choice of mathematical structure (`direct`, `factor`, or `additive`).\n",
    "* **`--iterations` (int, default: 150)**: The depth of the evolutionary search.\n",
    "* **`--feature_set` (str)**: Defines which physiological variables to include (e.g., `rr_only` or `extended` including age/gender).\n",
    "\n",
    "#### Output\n",
    "\n",
    "The module generates a comprehensive research package in the results folder:\n",
    "\n",
    "* **`task4_sr_report.json`**: A master file containing the \"Best Overall\" equation, its mathematical complexity, and the achieved loss.\n",
    "* **`sr_results_[approach].csv`**: The full \"Hall of Fame\" (Pareto front) for each approach, listing candidate equations from simplest to most accurate.\n",
    "* **Comparison Metrics**:\n",
    "* **|r(HR)|**: The absolute Pearson correlation between the new QTc and Heart Rate (lower is better).\n",
    "* **Improvement Factor**: A quantitative measure of how much better the new formula performs compared to the Bazett baseline.\n",
    "\n",
    "\n",
    "* **Visualizations**:\n",
    "* **Pareto Front Plot**: Shows the trade-off between equation complexity and error.\n",
    "* **Correlation Charts**: Visualizes the decoupling of QT from Heart Rate using the new formula.\n",
    "\n",
    "#### Running\n",
    "\n",
    "```bash\n",
    "python scripts/sr_qtc_discovery.py --dataset ptb-xl --approach all\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b629f471",
   "metadata": {},
   "source": [
    "### Step 4.7: QTc Formula Validation & Clinical Benchmarking\n",
    "\n",
    "**Script**: [scripts/validate_qtc_formulas.py](../scripts/validate_qtc_formulas.py)\n",
    "\n",
    "The **QTc Formula Validation** module is a scientific verification utility within the Kepler-ECG pipeline. Its primary purpose is to rigorously assess the \"Heart Rate Independence\" of discovered mathematical formulas compared to traditional clinical standards (Bazett, Fridericia, Framingham, and Hodges). It evaluates how well a formula \"decouples\" the QT interval from the heart rate across diverse physiological ranges (Bradycardia, Normal, and Tachycardia). This script provides the statistical proof required to demonstrate that the new Kepler formulas offer superior stability and more accurate diagnostic thresholds for identifying patients at risk of Arrhythmia.\n",
    "\n",
    "#### Input\n",
    "\n",
    "The script operates on cleaned, high-resolution feature data:\n",
    "\n",
    "* **`--dataset` (str)**: The name of the registered dataset (e.g., `'ptb-xl'`) to automatically fetch the SR-ready CSV.\n",
    "* **`--input` (Path)**: Alternatively, a direct path to a CSV (e.g., `qtc_sr_dataset_all.csv`) containing QT intervals, RR intervals, and diagnostic labels.\n",
    "* **Predefined Kepler Formulas**: The script internally defines the mathematical expressions for discovered models like \"Kepler-Cubic\" or \"Kepler-Linear\" to test them against the population.\n",
    "\n",
    "#### Output\n",
    "\n",
    "The module generates a comprehensive validation package in the `validation/` directory:\n",
    "\n",
    "* **`qtc_validation_report.json`**: A master file containing:\n",
    "* **HR Independence Ranking**: All formulas ranked by the absolute Pearson correlation () with heart rate (lower is better).\n",
    "* **Bin Analysis**: Statistical stability of each formula across specific BPM ranges.\n",
    "* **Clinical Threshold Analysis**: Percentage of patients classified as \"Prolonged QT\" (>450ms) or \"High Risk\" (>500ms) by each formula.\n",
    "\n",
    "\n",
    "* **`validation_summary.png`**: A high-resolution plot showing the regression of QTc vs. Heart Rate for all competing models.\n",
    "* **`hr_bin_comparison.png`**: A box-plot visualization showing the distribution of corrected intervals across different heart rate categories.\n",
    "* **Console Leaderboard**: A final summary highlighting the \"Improvement Factor\" of the best Kepler formula over the Bazett baseline.\n",
    "\n",
    "```bash\n",
    "python scripts/validate_qtc_formulas.py --dataset ptb-xl\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459f3b4a",
   "metadata": {},
   "source": [
    "### Step 4.8: QTc Discovery Report Generator\n",
    "\n",
    "**Script**: [scripts/generate_qtc_report.py](../scripts/generate_qtc_report.py)\n",
    "\n",
    "## üìë Kepler-ECG: QTc Discovery Report Generator\n",
    "\n",
    "The **QTc Discovery Report Generator** is the final documentation engine for the **Phase 4.5** research stream. It serves as a scientific synthesizer, automatically gathering technical outputs from wave delineation, symbolic regression, and clinical validation tasks to produce a human-readable research summary. The script generates multiple formats (JSON, Markdown, and Clinical Interpretation) to facilitate both further automated processing and peer-reviewed publication. It specifically focuses on quantifying the \"Improvement Factor\" of the discovered Kepler formulas over century-old standards like Bazett's formula.\n",
    "\n",
    "#### Input\n",
    "\n",
    "The script functions as an aggregator, looking for JSON artifacts generated in previous pipeline steps:\n",
    "\n",
    "* **`--validation` (Path)**: The validation JSON (from Task 5) containing HR-independence rankings and clinical threshold data.\n",
    "* **`--sr` (Path)**: The Symbolic Regression report (from Task 4) detailing the best-discovered equations and their mathematical complexity.\n",
    "* **`--waves` (Path)**: The wave delineation summary (from Task 1) providing population statistics and processing success rates.\n",
    "* **`--dataset` (str)**: Used to automatically resolve file paths if the standard project structure is maintained.\n",
    "\n",
    "#### Output\n",
    "\n",
    "The module produces a suite of \"Publication-Ready\" documents:\n",
    "\n",
    "* **`qtc_final_report.json`**: A master data file containing every metric, formula, and performance benchmark from the stream.\n",
    "* **`qtc_final_report.md`**: A formatted Markdown report suitable for GitHub documentation or laboratory notebooks.\n",
    "* **`clinical_interpretation.md`**: A medically-focused document explaining the rationale behind the new formulas and their potential impact on misdiagnosis rates in Tachycardia/Bradycardia.\n",
    "* **Executive Summary**: A console-based report highlighting the \"Success Status\" (Pass/Fail) based on the project's predefined scientific targets.\n",
    "\n",
    "```bash\n",
    "python scripts/generate_qtc_report.py --dataset ptb-xl\n",
    "```\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
